# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-strict
"""
mlperf dlrm_v3 inference benchmarking tool.
"""

import argparse
import array
import json
import logging

logging.basicConfig(level=logging.INFO)
import math
import os
import sys
import time
from typing import Any, Dict, List, Optional, Union

import gin

# pyre-ignore [21]
import mlperf_loadgen as lg  # @manual
import numpy as np
import torch
from generative_recommenders.common import set_dev_mode, set_verbose_level
from generative_recommenders.dlrm_v3.configs import (
    get_embedding_table_config,
    get_hstu_configs,
)
from generative_recommenders.dlrm_v3.datasets.dataset import Dataset
from generative_recommenders.dlrm_v3.inference.data_producer import (
    MultiThreadDataProducer,
    QueryItem,
    SingleThreadDataProducer,
)
from generative_recommenders.dlrm_v3.inference.inference_modules import set_is_inference
from generative_recommenders.dlrm_v3.inference.model_family import HSTUModelFamily
from generative_recommenders.dlrm_v3.utils import (
    get_dataset,
    MetricsLogger,
    profiler_or_nullcontext,
    SUPPORTED_DATASETS,
)


logger: logging.Logger = logging.getLogger("main")

torch.multiprocessing.set_start_method("spawn", force=True)

NANO_SEC = 1e9

MLPERF_CONF = f"{os.path.dirname(__file__)}/mlperf.conf"
USER_CONF = f"{os.path.dirname(__file__)}/user.conf"

SUPPORTED_CONFIGS = {
    "debug": "debug.gin",
    "kuairand-1k": "kuairand_1k.gin",
}


SCENARIO_MAP = {  # pyre-ignore [5]
    "SingleStream": lg.TestScenario.SingleStream,
    "MultiStream": lg.TestScenario.MultiStream,
    "Server": lg.TestScenario.Server,
    "Offline": lg.TestScenario.Offline,
}


def get_args():  # pyre-ignore [3]
    """Parse commandline."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--dataset", default="debug", choices=SUPPORTED_DATASETS, help="dataset"
    )
    args, unknown_args = parser.parse_known_args()
    logger.warning(f"unknown_args: {unknown_args}")
    return args


class Runner:
    def __init__(
        self,
        model: HSTUModelFamily,
        ds: Dataset,
        data_producer_threads: int = 1,
        batchsize: int = 128,
        compute_eval: bool = False,
    ) -> None:
        self.model = model
        if data_producer_threads == 1:
            self.data_producer: Union[
                MultiThreadDataProducer, SingleThreadDataProducer
            ] = SingleThreadDataProducer(ds, self.run_one_item)
        else:
            self.data_producer = MultiThreadDataProducer(
                ds, data_producer_threads, self.run_one_item
            )
        self.batchsize = batchsize
        self.compute_eval = compute_eval
        self.result_timing: List[float] = []
        self.result_batches: List[int] = []
        self.current_query_ids: List[int] = []
        self.current_content_ids: List[int] = []
        self.metrics: Optional[MetricsLogger] = None
        if compute_eval:
            self.metrics = MetricsLogger(
                multitask_configs=model.hstu_config.multitask_configs,  # pyre-ignore [6]
                batch_size=batchsize,
                window_size=1000,
                device=torch.device("cpu"),
                rank=0,
            )

    def run_one_item(self, qitem: QueryItem) -> None:
        try:
            prediction_output = self.model.predict(qitem.samples)
            assert prediction_output is not None
            mt_target_preds, mt_target_labels, mt_target_weights = prediction_output
            if self.compute_eval:
                assert mt_target_labels is not None
                assert mt_target_weights is not None
                assert self.metrics is not None
                self.metrics.update(
                    predictions=mt_target_preds.t(),
                    labels=mt_target_labels.t(),
                    weights=mt_target_weights.t(),
                )
            self.result_timing.append(time.time() - qitem.start)
            self.result_batches.append(len(qitem.query_ids))
        except Exception as ex:  # pylint: disable=broad-except
            logger.error("thread: failed, %s", ex)
        finally:
            response = []
            for query_id in qitem.query_ids:
                response_array = array.array(
                    "B", np.array([1]).astype(np.float32).tobytes()
                )
                bi = response_array.buffer_info()
                response.append(lg.QuerySampleResponse(query_id, bi[0], bi[1]))
            lg.QuerySamplesComplete(response)

    def enqueue(self, query_samples) -> None:  # pyre-ignore [2]
        self.current_query_ids.extend([q.id for q in query_samples])
        self.current_content_ids.extend([q.index for q in query_samples])
        if len(self.current_query_ids) >= self.batchsize:
            self.data_producer.enqueue(self.current_query_ids, self.current_content_ids)
            self.current_query_ids = []
            self.current_content_ids = []

    def finish(self) -> None:
        self.data_producer.finish()


def add_results(
    final_results: Dict[str, Any],
    result_timing: List[float],
    result_batches: List[int],
    metrics: Optional[MetricsLogger],
) -> None:
    percentiles = [50.0, 80.0, 90.0, 95.0, 99.0, 99.9]
    buckets = np.percentile(result_timing, percentiles).tolist()
    buckets_str = ",".join(
        ["{}:{:.4f}".format(p, b) for p, b in zip(percentiles, buckets)]
    )
    total_batches = sum(result_batches)

    final_results["good"] = len(result_timing)
    final_results["avg_time"] = np.mean(result_timing)
    final_results["percentiles"] = {str(k): v for k, v in zip(percentiles, buckets)}
    final_results["qps"] = total_batches / final_results["took"]
    final_results["count"] = total_batches

    if metrics is not None:
        for k, v in metrics.compute().items():
            print(f"{k}: {v}")

    print(
        "{} qps={:.2f}, avg_query_time={:.4f}, time={:.3f}, queries={}, tiles={}".format(
            final_results["scenario"],
            final_results["qps"],
            final_results["avg_time"],
            final_results["took"],
            len(result_timing),
            buckets_str,
        )
    )


@gin.configurable
def run(
    dataset: str = "debug",
    model_path: str = "",
    scenario_name: str = "Server",
    batchsize: int = 16,
    out_dir: str = "",
    output_trace: bool = False,
    data_producer_threads: int = 4,
    compute_eval: bool = False,
    find_peak_performance: bool = False,
    new_path_prefix: str = "",
    train_split_percentage: float = 0.75,
    # below will override mlperf rules compliant settings - don't use for official submission
    duration: Optional[int] = None,
    target_qps: Optional[int] = None,
    max_latency: Optional[float] = None,
    num_queries: Optional[int] = None,
    samples_per_query_multistream: int = 8,
    max_num_samples: int = 2048,
    numpy_rand_seed: int = 123,
    dev_mode: bool = False,
) -> None:
    set_dev_mode(dev_mode)
    if scenario_name not in SCENARIO_MAP:
        raise NotImplementedError("valid scanarios:" + str(list(SCENARIO_MAP.keys())))
    np.random.seed(numpy_rand_seed)

    hstu_config = get_hstu_configs(dataset)
    table_config = get_embedding_table_config(dataset)
    set_is_inference(is_inference=not compute_eval)

    model_family = HSTUModelFamily(
        hstu_config=hstu_config,
        table_config=table_config,
        output_trace=output_trace,
    )
    dataset, kwargs = get_dataset(dataset, new_path_prefix)

    ds: Dataset = dataset(
        hstu_config=hstu_config,
        embedding_config=table_config,
        is_inference=not compute_eval,
        **kwargs,
    )
    model_family.load(model_path)

    mlperf_conf = os.path.abspath(MLPERF_CONF)
    if not os.path.exists(mlperf_conf):
        logger.error("{} not found".format(mlperf_conf))
        sys.exit(1)
    user_conf = os.path.abspath(USER_CONF)
    if not os.path.exists(user_conf):
        logger.error("{} not found".format(user_conf))
        sys.exit(1)

    if out_dir:
        output_dir = os.path.abspath(out_dir)
        os.makedirs(output_dir, exist_ok=True)
        os.chdir(output_dir)

    scenario = SCENARIO_MAP[scenario_name]
    if scenario != lg.TestScenario.Server or compute_eval:
        batchsize = 1

    # warmup
    warmup_ids = list(range(batchsize))
    ds.load_query_samples(warmup_ids)
    for _ in range(5 * int(os.environ.get("WORLD_SIZE", 1))):
        sample = ds.get_samples(warmup_ids)
        _ = model_family.predict(sample)
    ds.unload_query_samples(None)
    for h in logger.handlers:
        h.flush()
    logger.info("warmup done")

    count = ds.get_item_count()
    train_size: int = round(train_split_percentage * count)

    settings = lg.TestSettings()
    settings.FromConfig(mlperf_conf, model_path, scenario_name)
    settings.FromConfig(user_conf, model_path, scenario_name)
    settings.scenario = scenario
    settings.mode = lg.TestMode.PerformanceOnly

    if compute_eval:
        settings.mode = lg.TestMode.AccuracyOnly
        count = count - train_size
        data_producer_threads = 1  # during eval, using multiple threads can have concurrency issue due to Triton autotune.

    runner: Runner = Runner(
        model_family,
        ds,
        data_producer_threads=data_producer_threads,
        batchsize=batchsize,
        compute_eval=compute_eval,
    )

    def issue_queries(query_samples) -> None:  # pyre-ignore [2]
        if compute_eval:
            for sample in query_samples:
                sample.index = sample.index + train_size
        runner.enqueue(query_samples)

    def load_query_samples(query_ids: List[int]) -> None:
        if compute_eval:
            query_ids = [q + train_size for q in query_ids]
        ds.load_query_samples(query_ids)

    def flush_queries() -> None:
        pass

    if find_peak_performance:
        settings.mode = lg.TestMode.FindPeakPerformance

    if duration:
        settings.min_duration_ms = duration
        settings.max_duration_ms = duration

    if target_qps:
        settings.server_target_qps = float(target_qps)
        settings.offline_expected_qps = float(target_qps)

    if num_queries:
        queries = math.ceil(num_queries / batchsize) * batchsize
        settings.min_query_count = queries
        settings.max_query_count = queries

    if samples_per_query_multistream:
        settings.multi_stream_samples_per_query = samples_per_query_multistream

    if max_latency:
        settings.server_target_latency_ns = int(max_latency * NANO_SEC)
        settings.multi_stream_expected_latency_ns = int(max_latency * NANO_SEC)

    sut = lg.ConstructSUT(issue_queries, flush_queries)
    qsl = lg.ConstructQSL(
        count,
        min(count, max_num_samples),
        load_query_samples,
        ds.unload_query_samples,
    )

    final_results = {
        "runtime": model_family.name(),
        "version": model_family.version(),
        "time": int(time.time()),
        "scenario": str(scenario),
    }

    with profiler_or_nullcontext(enabled=output_trace, with_stack=False):
        logger.info(f"starting {scenario}")
        lg.StartTest(sut, qsl, settings)
        runner.finish()
        final_results["took"] = time.time() - ds.last_loaded
        lg.DestroyQSL(qsl)
        lg.DestroySUT(sut)

    add_results(
        final_results,
        runner.result_timing,
        runner.result_batches,
        runner.metrics,
    )
    # If multiple subprocesses are running the model send a signal to stop them
    if int(os.environ.get("WORLD_SIZE", 1)) > 1:
        model_family.predict(None)

    if out_dir:
        with open("results.json", "w") as f:
            json.dump(final_results, f, sort_keys=True, indent=4)


def main() -> None:
    set_verbose_level(1)
    args = get_args()
    logger.info(args)
    gin_path = f"{os.path.dirname(__file__)}/gin/{SUPPORTED_CONFIGS[args.dataset]}"
    gin.parse_config_file(gin_path)
    run(dataset=args.dataset)


if __name__ == "__main__":
    main()
