# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3

# pyre-strict

from math import sqrt
from typing import Callable, Dict, Optional, Tuple

import torch
from generative_recommenders.common import (
    fx_infer_max_len,
    fx_unwrap_optional_tensor,
    HammerKernel,
)
from generative_recommenders.modules.action_encoder import ActionEncoder
from generative_recommenders.modules.content_encoder import ContentEncoder
from generative_recommenders.modules.contextualize_mlps import (
    ContextualizedMLP,
    ParameterizedContextualizedMLP,
)
from generative_recommenders.modules.preprocessors import (
    get_contextual_input_embeddings,
    InputPreprocessor,
)
from generative_recommenders.ops.jagged_tensors import concat_2D_jagged


def _combine_embeddings(
    output_embedding_dim: int,
    max_seq_len: int,
    seq_lengths: torch.Tensor,
    seq_timestamps: torch.Tensor,
    content_embeddings: torch.Tensor,
    action_embeddings: torch.Tensor,
    max_contextual_seq_len: int,
    contextual_embeddings: Optional[torch.Tensor],
    num_targets: torch.Tensor,
    enable_interleaving: bool,
    interleave_targets: bool,
    kernel: HammerKernel,
) -> Tuple[
    int,
    torch.Tensor,
    torch.Tensor,
    torch.Tensor,
    torch.Tensor,
    torch.Tensor,
]:
    if enable_interleaving:
        output_seq_timestamps = seq_timestamps.repeat_interleave(2)
        output_seq_embeddings = torch.stack(
            [content_embeddings, action_embeddings], dim=1
        ).reshape(-1, output_embedding_dim)
        if interleave_targets:
            output_seq_lengths = seq_lengths * 2
            output_max_seq_len = max_seq_len * 2
            output_num_targets = num_targets * 2
        else:
            seq_lengths_by_2 = seq_lengths * 2
            output_seq_lengths = seq_lengths_by_2 - num_targets
            output_max_seq_len = fx_infer_max_len(output_seq_lengths)
            indices = torch.arange(2 * max_seq_len, device=seq_lengths.device).view(
                1, -1
            )
            valid_mask = torch.logical_and(
                indices < seq_lengths_by_2.view(-1, 1),
                torch.logical_or(
                    indices < (output_seq_lengths - num_targets).view(-1, 1),
                    torch.remainder(indices, 2) == 0,
                ),
            )
            jagged_valid_mask = (
                torch.ops.fbgemm.dense_to_jagged(
                    valid_mask.int().unsqueeze(-1),
                    [torch.ops.fbgemm.asynchronous_complete_cumsum(seq_lengths_by_2)],
                )[0]
                .to(torch.bool)
                .squeeze(1)
            )
            output_seq_embeddings = output_seq_embeddings[jagged_valid_mask]
            output_seq_timestamps = output_seq_timestamps[jagged_valid_mask]
            output_num_targets = num_targets
    else:
        output_max_seq_len = max_seq_len
        output_seq_lengths = seq_lengths
        output_num_targets = num_targets
        output_seq_timestamps = seq_timestamps
        output_seq_embeddings = content_embeddings + action_embeddings

    # concat contextual embeddings
    output_seq_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(
        output_seq_lengths
    )
    if max_contextual_seq_len > 0:
        output_seq_embeddings = concat_2D_jagged(
            values_left=fx_unwrap_optional_tensor(contextual_embeddings).reshape(
                -1, output_embedding_dim
            ),
            values_right=output_seq_embeddings,
            max_len_left=max_contextual_seq_len,
            max_len_right=output_max_seq_len,
            offsets_left=None,
            offsets_right=output_seq_offsets,
            kernel=kernel,
        )
        output_seq_timestamps = concat_2D_jagged(
            values_left=torch.zeros(
                (output_seq_lengths.size(0) * max_contextual_seq_len, 1),
                dtype=output_seq_timestamps.dtype,
                device=output_seq_timestamps.device,
            ),
            values_right=output_seq_timestamps.unsqueeze(-1),
            max_len_left=max_contextual_seq_len,
            max_len_right=output_max_seq_len,
            offsets_left=None,
            offsets_right=output_seq_offsets,
            kernel=kernel,
        ).squeeze(-1)
        output_max_seq_len = output_max_seq_len + max_contextual_seq_len
        output_seq_lengths = output_seq_lengths + max_contextual_seq_len
        output_seq_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(
            output_seq_lengths
        )

    return (
        output_max_seq_len,
        output_seq_lengths,
        output_seq_offsets,
        output_seq_timestamps,
        output_seq_embeddings,
        output_num_targets,
    )


class ContextualInterleavePreprocessor(InputPreprocessor):
    def __init__(
        self,
        input_embedding_dim: int,
        output_embedding_dim: int,
        contextual_feature_to_max_length: Dict[str, int],
        contextual_feature_to_min_uih_length: Dict[str, int],
        content_encoder: ContentEncoder,
        content_contextualize_mlp_fn: Callable[
            [int, int, int, bool], ContextualizedMLP
        ],
        action_encoder: ActionEncoder,
        action_contextualize_mlp_fn: Callable[[int, int, int, bool], ContextualizedMLP],
        pmlp_contextual_dropout_ratio: float = 0.0,
        enable_interleaving: bool = False,
        is_inference: bool = False,
    ) -> None:
        super().__init__(is_inference=is_inference)
        self._input_embedding_dim: int = input_embedding_dim
        self._output_embedding_dim: int = output_embedding_dim
        self._contextual_feature_to_max_length: Dict[str, int] = (
            contextual_feature_to_max_length
        )
        self._max_contextual_seq_len: int = sum(
            contextual_feature_to_max_length.values()
        )
        self._contextual_feature_to_min_uih_length: Dict[str, int] = (
            contextual_feature_to_min_uih_length
        )
        std = 1.0 * sqrt(2.0 / float(input_embedding_dim + output_embedding_dim))
        self._batched_contextual_linear_weights = torch.nn.Parameter(
            torch.empty(
                (
                    self._max_contextual_seq_len,
                    input_embedding_dim,
                    output_embedding_dim,
                )
            ).normal_(0.0, std)
        )
        self._pmlp_contextual_dropout_ratio: float = pmlp_contextual_dropout_ratio
        self._batched_contextual_linear_bias = torch.nn.Parameter(
            torch.empty((self._max_contextual_seq_len, 1, output_embedding_dim)).fill_(
                0.0
            )
        )
        contextual_embedding_dim: int = (
            self._max_contextual_seq_len * input_embedding_dim
        )
        self._content_encoder: ContentEncoder = content_encoder
        self._content_embedding_mlp: ContextualizedMLP = content_contextualize_mlp_fn(
            self._content_encoder.output_embedding_dim,
            output_embedding_dim,
            contextual_embedding_dim,
            is_inference,
        )
        self._action_encoder: ActionEncoder = action_encoder
        self._action_embedding_mlp: ContextualizedMLP = action_contextualize_mlp_fn(
            self._action_encoder.output_embedding_dim,
            output_embedding_dim,
            contextual_embedding_dim,
            is_inference,
        )
        self._enable_interleaving: bool = enable_interleaving

    def combine_embeddings(
        self,
        max_seq_len: int,
        seq_lengths: torch.Tensor,
        seq_timestamps: torch.Tensor,
        content_embeddings: torch.Tensor,
        action_embeddings: torch.Tensor,
        contextual_embeddings: Optional[torch.Tensor],
        num_targets: torch.Tensor,
    ) -> Tuple[
        int,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
    ]:
        if self._enable_interleaving:
            output_seq_timestamps = seq_timestamps.repeat_interleave(2)
            output_seq_embeddings = torch.stack(
                [content_embeddings, action_embeddings], dim=1
            ).reshape(-1, self._output_embedding_dim)
            if self.interleave_targets():
                output_seq_lengths = seq_lengths * 2
                output_max_seq_len = max_seq_len * 2
                output_num_targets = num_targets * 2
            else:
                seq_lengths_by_2 = seq_lengths * 2
                output_seq_lengths = seq_lengths_by_2 - num_targets
                output_max_seq_len = fx_infer_max_len(output_seq_lengths)
                indices = torch.arange(2 * max_seq_len, device=seq_lengths.device).view(
                    1, -1
                )
                valid_mask = torch.logical_and(
                    indices < seq_lengths_by_2.view(-1, 1),
                    torch.logical_or(
                        indices < (output_seq_lengths - num_targets).view(-1, 1),
                        torch.remainder(indices, 2) == 0,
                    ),
                )
                jagged_valid_mask = (
                    torch.ops.fbgemm.dense_to_jagged(
                        valid_mask.int().unsqueeze(-1),
                        [
                            torch.ops.fbgemm.asynchronous_complete_cumsum(
                                seq_lengths_by_2
                            )
                        ],
                    )[0]
                    .to(torch.bool)
                    .squeeze(1)
                )
                output_seq_embeddings = output_seq_embeddings[jagged_valid_mask]
                output_seq_timestamps = output_seq_timestamps[jagged_valid_mask]
                output_num_targets = num_targets
        else:
            output_max_seq_len = max_seq_len
            output_seq_lengths = seq_lengths
            output_num_targets = num_targets
            output_seq_timestamps = seq_timestamps
            output_seq_embeddings = content_embeddings + action_embeddings

        # concat contextual embeddings
        output_seq_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(
            output_seq_lengths
        )
        if self._max_contextual_seq_len > 0:
            output_seq_embeddings = concat_2D_jagged(
                values_left=fx_unwrap_optional_tensor(contextual_embeddings).reshape(
                    -1, self._output_embedding_dim
                ),
                values_right=output_seq_embeddings,
                max_len_left=self._max_contextual_seq_len,
                max_len_right=output_max_seq_len,
                offsets_left=None,
                offsets_right=output_seq_offsets,
                kernel=self.hammer_kernel(),
            )
            output_seq_timestamps = concat_2D_jagged(
                values_left=torch.zeros(
                    (output_seq_lengths.size(0) * self._max_contextual_seq_len, 1),
                    dtype=output_seq_timestamps.dtype,
                    device=output_seq_timestamps.device,
                ),
                values_right=output_seq_timestamps.unsqueeze(-1),
                max_len_left=self._max_contextual_seq_len,
                max_len_right=output_max_seq_len,
                offsets_left=None,
                offsets_right=output_seq_offsets,
                kernel=self.hammer_kernel(),
            ).squeeze(-1)
            output_max_seq_len = output_max_seq_len + self._max_contextual_seq_len
            output_seq_lengths = output_seq_lengths + self._max_contextual_seq_len
            output_seq_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(
                output_seq_lengths
            )

        return (
            output_max_seq_len,
            output_seq_lengths,
            output_seq_offsets,
            output_seq_timestamps,
            output_seq_embeddings,
            output_num_targets,
        )

    def forward(  # noqa C901
        self,
        max_seq_len: int,
        seq_lengths: torch.Tensor,
        seq_timestamps: torch.Tensor,
        seq_embeddings: torch.Tensor,
        num_targets: torch.Tensor,
        seq_payloads: Dict[str, torch.Tensor],
    ) -> Tuple[
        int,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        Dict[str, torch.Tensor],
    ]:
        # get contextual_embeddings
        contextual_embeddings: Optional[torch.Tensor] = None
        pmlp_contextual_embeddings: Optional[torch.Tensor] = None
        if self._max_contextual_seq_len > 0:
            contextual_input_embeddings = get_contextual_input_embeddings(
                seq_lengths=seq_lengths,
                seq_payloads=seq_payloads,
                contextual_feature_to_max_length=self._contextual_feature_to_max_length,
                contextual_feature_to_min_uih_length=self._contextual_feature_to_min_uih_length,
                dtype=seq_embeddings.dtype,
            )
            if isinstance(
                self._action_embedding_mlp, ParameterizedContextualizedMLP
            ) or isinstance(self._action_embedding_mlp, ParameterizedContextualizedMLP):
                pmlp_contextual_embeddings = torch.nn.functional.dropout(
                    contextual_input_embeddings,
                    p=self._pmlp_contextual_dropout_ratio,
                    training=self.training,
                )
            contextual_embeddings = torch.baddbmm(
                self._batched_contextual_linear_bias.to(
                    contextual_input_embeddings.dtype
                ),
                contextual_input_embeddings.view(
                    -1, self._max_contextual_seq_len, self._input_embedding_dim
                ).transpose(0, 1),
                self._batched_contextual_linear_weights.to(
                    contextual_input_embeddings.dtype
                ),
            ).transpose(0, 1)

        # content embeddings
        seq_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(seq_lengths)
        content_embeddings = self._content_encoder(
            max_seq_len=max_seq_len,
            seq_embeddings=seq_embeddings,
            seq_lengths=seq_lengths,
            seq_offsets=seq_offsets,
            seq_payloads=seq_payloads,
            num_targets=num_targets,
        )
        content_embeddings = self._content_embedding_mlp(
            seq_embeddings=content_embeddings,
            seq_offsets=seq_offsets,
            max_seq_len=max_seq_len,
            contextual_embeddings=pmlp_contextual_embeddings,
        )

        # action embeddings
        action_embeddings = self._action_encoder(
            max_seq_len=max_seq_len,
            seq_lengths=seq_lengths,
            seq_offsets=seq_offsets,
            seq_payloads=seq_payloads,
            num_targets=num_targets,
        )
        action_embeddings = self._action_embedding_mlp(
            seq_embeddings=action_embeddings,
            seq_offsets=seq_offsets,
            max_seq_len=max_seq_len,
            contextual_embeddings=pmlp_contextual_embeddings,
        )

        (
            output_max_seq_len,
            output_seq_lengths,
            output_seq_offsets,
            output_seq_timestamps,
            output_seq_embeddings,
            output_num_targets,
        ) = self.combine_embeddings(
            max_seq_len=max_seq_len,
            seq_lengths=seq_lengths,
            seq_timestamps=seq_timestamps,
            content_embeddings=content_embeddings,
            action_embeddings=action_embeddings,
            contextual_embeddings=contextual_embeddings,
            num_targets=num_targets,
        )

        return (
            output_max_seq_len,
            output_seq_lengths,
            output_seq_offsets,
            output_seq_timestamps,
            output_seq_embeddings,
            output_num_targets,
            seq_payloads,
        )

    def interleave_targets(self) -> bool:
        return self.is_train and self._enable_interleaving
