# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Frozen config, validated on 04/12/2024.
# Based on baseline settings in Revisiting Neural Retrieval on Accelerators (https://arxiv.org/abs/2306.04039, KDD'23).
#
# Run this as:
# mkdir -p logs/amzn-books-l50/
# CUDA_VISIBLE_DEVICES=1 python3 train.py --gin_config_file=configs/amzn-books/sasrec-sampled-softmax-n512-final.gin  --master_port=12346 2>&1 | tee logs/amzn-books-l50/sasrec-sampled-softmax-n512-final.log

train_fn.dataset_name = "amzn-books"
train_fn.max_sequence_length = 50
train_fn.local_batch_size = 128
train_fn.eval_batch_size = 128

train_fn.main_module = "SASRec"
train_fn.dropout_rate = 0.5
train_fn.user_embedding_norm = "l2_norm"
train_fn.item_embedding_dim = 64

sasrec_encoder.num_blocks = 4
sasrec_encoder.num_heads = 4
sasrec_encoder.ffn_dropout_rate = 0.5
sasrec_encoder.ffn_hidden_dim = 64
sasrec_encoder.ffn_activation_fn = "relu"

train_fn.eval_interval = 4000
train_fn.num_epochs = 201
train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.save_ckpt_every_n = 10

train_fn.interaction_module_type = "DotProduct"
train_fn.top_k_method = "MIPSBruteForceTopK"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 512

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True
train_fn.full_eval_every_n = 5
train_fn.partial_eval_num_iters = 64

create_data_loader.prefetch_factor = 1024
create_data_loader.num_workers = 8
